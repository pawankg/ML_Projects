{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7yuytuIllsv1"
   },
   "source": [
    "\n",
    "# Transformer Summarizer\n",
    "\n",
    "In this assignment you will explore summarization using the transformer model. Yes, you will implement the transformer decoder from scratch, but we will slowly walk you through it. There are many hints in this notebook so feel free to use them as needed. \n",
    "\n",
    "<img src = \"transformerNews.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4-3lxSnXRWPx"
   },
   "source": [
    "## Outline\n",
    "\n",
    "- [Introduction](#0)\n",
    "- [Part 1: Importing the dataset](#1)\n",
    "    - [1.1 Encode & Decode helper functions](#1.1)\n",
    "    - [1.2 Defining parameters](#1.2)\n",
    "    - [1.3 Exploring the data](#1.3)\n",
    "- [Part 2: Summarization with transformer](#2)\n",
    "    - [2.1 Dot product attention](#2.1)\n",
    "        - [Exercise 01](#ex01)\n",
    "    - [2.2 Causal Attention](#2.2)\n",
    "        - [Exercise 02](#ex02)\n",
    "    - [2.3 Transformer decoder block](#2.3)\n",
    "        - [Exercise 03](#ex03)\n",
    "    - [2.4 Transformer Language model](#2.4)\n",
    "        - [Exercise 04](#ex04)\n",
    "- [Part 3: Training](#3)\n",
    "    - [3.1 Training the model](#3.1)\n",
    "        - [Exercise 05](#ex05)\n",
    "- [Part 4: Evaluation](#4)\n",
    "    - [4.1 Loading in a trained model](#4.1)\n",
    "- [Part 5: Testing with your own input](#5) \n",
    "    - [Exercise 6](#ex06)\n",
    "    - [5.1 Greedy decoding](#5.1)\n",
    "        - [Exercise 07](#ex07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H4NlfEQhRWPy"
   },
   "source": [
    "<a name='0'></a>\n",
    "### Introduction\n",
    "\n",
    "Summarization is an important task in natural language processing and could be useful for a consumer enterprise. For example, bots can be used to scrape articles, summarize them, and then you can use sentiment analysis to identify the sentiment about certain stocks. Anyways who wants to read an article or a long email today, when you can build a transformer to summarize text for you. Let's get started, by completing this assignment you will learn to:  \n",
    "\n",
    "- Use built-in functions to preprocess your data\n",
    "- Implement DotProductAttention\n",
    "- Implement Causal Attention\n",
    "- Understand how attention works\n",
    "- Build the transformer model\n",
    "- Evaluate your model\n",
    "- Summarize an article\n",
    "\n",
    "As you can tell, this model is slightly different than the ones you have already implemented. This is heavily based on attention and does not rely on sequences, which allows for parallel computing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CChWzW-rEHVb",
    "outputId": "a0b3e98b-7fc6-492d-c8ad-3a263b54f670"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=70)\n",
    "\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as jnp\n",
    "\n",
    "# to print the entire np array\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kEL2rvaHRWP4"
   },
   "source": [
    "<a name='1'></a>\n",
    "## Part 1: Importing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trax makes it easy to work with Tensorflow's datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VInmKSkhEhle"
   },
   "outputs": [],
   "source": [
    "# This will download the dataset if no data_dir is specified.\n",
    "# Downloading and processing can take bit of time,\n",
    "# so we have the data already in 'data/' for you\n",
    "\n",
    "# Importing CNN/DailyMail articles dataset\n",
    "train_stream_fn = trax.data.TFDS('cnn_dailymail',\n",
    "                                 data_dir='data/',\n",
    "                                 keys=('article', 'highlights'),\n",
    "                                 train=True)\n",
    "\n",
    "# This should be much faster as the data is downloaded already.\n",
    "eval_stream_fn = trax.data.TFDS('cnn_dailymail',\n",
    "                                data_dir='data/',\n",
    "                                keys=('article', 'highlights'),\n",
    "                                train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1.1'></a>\n",
    "## 1.1 Tokenize & Detokenize helper functions\n",
    "\n",
    "Just like in the previous assignment, the cell above loads in the encoder for you. Given any data set, you have to be able to map words to their indices, and indices to their words. The inputs and outputs to your [Trax](https://github.com/google/trax) models are usually tensors of numbers where each number corresponds to a word. If you were to process your data manually, you would have to make use of the following: \n",
    "\n",
    "- <span style='color:blue'> word2Ind: </span> a dictionary mapping the word to its index.\n",
    "- <span style='color:blue'> ind2Word:</span> a dictionary mapping the index to its word.\n",
    "- <span style='color:blue'> word2Count:</span> a dictionary mapping the word to the number of times it appears. \n",
    "- <span style='color:blue'> num_words:</span> total number of words that have appeared. \n",
    "\n",
    "Since you have already implemented these in previous assignments of the specialization, we will provide you with helper functions that will do this for you. Run the cell below to get the following functions:\n",
    "\n",
    "- <span style='color:blue'> tokenize: </span> converts a text sentence to its corresponding token list (i.e. list of indices). Also converts words to subwords.\n",
    "- <span style='color:blue'> detokenize: </span> converts a token list to its corresponding sentence (i.e. string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "djTiSLcaNFGa"
   },
   "outputs": [],
   "source": [
    "def tokenize(input_str, EOS=1):\n",
    "    \"\"\"Input str to features dict, ready for inference\"\"\"\n",
    "  \n",
    "    # Use the trax.data.tokenize method. It takes streams and returns streams,\n",
    "    # we get around it by making a 1-element stream with `iter`.\n",
    "    inputs =  next(trax.data.tokenize(iter([input_str]),\n",
    "                                      vocab_dir='vocab_dir/',\n",
    "                                      vocab_file='summarize32k.subword.subwords'))\n",
    "    \n",
    "    # Mark the end of the sentence with EOS\n",
    "    return list(inputs) + [EOS]\n",
    "\n",
    "def detokenize(integers):\n",
    "    \"\"\"List of ints to str\"\"\"\n",
    "  \n",
    "    s = trax.data.detokenize(integers,\n",
    "                             vocab_dir='vocab_dir/',\n",
    "                             vocab_file='summarize32k.subword.subwords')\n",
    "    \n",
    "    return wrapper.fill(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7WvhaFbCRWQS"
   },
   "source": [
    "<a name='1.2'></a>\n",
    "\n",
    "## 1.2 Preprocessing for Language Models: Concatenate It!\n",
    "\n",
    "This week you will use a language model -- Transformer Decoder -- to solve\n",
    "an input-output problem. As you know, language models only predict the next\n",
    "word, they have no notion of inputs. To create a single input suitable for\n",
    "a language model, we concatenate inputs with targets putting a separator\n",
    "in between. We also need to create a mask -- with 0s at inputs and 1s at targets -- so that the model is not penalized for mis-predicting the article and only focuses on the summary. See the preprocess function below for how this is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c4rgPxYSRWQS"
   },
   "outputs": [],
   "source": [
    "# Special tokens\n",
    "SEP = 0 # Padding or separator token\n",
    "EOS = 1 # End of sentence token\n",
    "\n",
    "# Concatenate tokenized inputs and targets using 0 as separator.\n",
    "def preprocess(stream):\n",
    "    for (article, summary) in stream:\n",
    "        joint = np.array(list(article) + [EOS, SEP] + list(summary) + [EOS])\n",
    "        mask = [0] * (len(list(article)) + 2) + [1] * (len(list(summary)) + 1) # Accounting for EOS and SEP\n",
    "        yield joint, joint, np.array(mask)\n",
    "\n",
    "# You can combine a few data preprocessing steps into a pipeline like this.\n",
    "input_pipeline = trax.data.Serial(\n",
    "    # Tokenizes\n",
    "    trax.data.Tokenize(vocab_dir='vocab_dir/',\n",
    "                       vocab_file='summarize32k.subword.subwords'),\n",
    "    # Uses function defined above\n",
    "    preprocess,\n",
    "    # Filters out examples longer than 2048\n",
    "    trax.data.FilterByLength(2048)\n",
    ")\n",
    "\n",
    "# Apply preprocessing to data streams.\n",
    "train_stream = input_pipeline(train_stream_fn())\n",
    "eval_stream = input_pipeline(eval_stream_fn())\n",
    "\n",
    "train_input, train_target, train_mask = next(train_stream)\n",
    "\n",
    "assert sum((train_input - train_target)**2) == 0  # They are the same in Language Model (LM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "uKFoGsUKSa_I",
    "outputId": "bc4d6634-d716-4311-d49c-1956bca2bc2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single example mask:\n",
      "\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# prints mask, 0s on article, 1s on summary\n",
    "print(f'Single example mask:\\n\\n {train_mask}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "S4uHyCkbSuUo",
    "outputId": "52845be8-f2fc-4803-bf7a-ed9725fe2bac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single example:\n",
      "\n",
      " Standoff: Goerge Pickering, pictured, allegedly threatened to kill a\n",
      "nurse . A distraught father who caused a four-hour standoff with\n",
      "police in a Texas hospital allegedly pointed his gun at a nurse and\n",
      "yelled 'I'll kill all of y'all'. Police said George Pickering, 57,\n",
      "made the threats from a hospital room after becoming inconsolable over\n",
      "the treatment of his son - a patient in critical care at Tomball\n",
      "Regional Hospital, near Houston. Armed police and a SWAT team\n",
      "descended on the medical center - and eventually convinced Pickering\n",
      "to surrender after a four-hour standoff on Saturday night. Pickering\n",
      "was charged with aggravated assault with a deadly weapon and is being\n",
      "held on a $30,000 bond, a statement from the Tomball Police department\n",
      "said. Detectives said Pickering was in the room with his son and\n",
      "family, waited for a nurse to come, then aimed his 9mm pistol at her.\n",
      "He then allegedly barricaded the room and threatened to kill anybody\n",
      "who came in. At the start of the confrontation, another of Pickering's\n",
      "sons, who was with him in the hospital room, allegedly wrested the gun\n",
      "away from him and handed it to police. Standoff: George Pickering, 57,\n",
      "allegedly threatened to kill a nurse with his pistol at Tomball\n",
      "Regional Hospital, sparking a police standoff . Response: Police and a\n",
      "SWAT team arrived at the hospital. Pickering reportedly had one gun\n",
      "taken from him, but said he had a second . Pickering then allegedly\n",
      "said, 'You don't think that's the only weapon I have?', prompting\n",
      "fears of a second gun and causing the lengthy showdown with police.\n",
      "But when he gave himself up, police found that he was not in fact\n",
      "armed. Early reports stated that Pickering had taken two hostages, but\n",
      "law enforcement later said there were no captives. A spokesman for the\n",
      "Tomball police department said Picerking fell ill during the standoff\n",
      "and was treated in the hospital overnight - and was still there Sunday\n",
      "afternoon. He does not yet have an attorney. Tense: Pickering was said\n",
      "to be distraught over the condition of his son - a critical care\n",
      "patient .<EOS><pad>GeorgePickering, 57, allegedly made threat from\n",
      "hospital room . Police said he aimed 9mm pistol at nurse inside\n",
      "Tomball Regional Hospital . Gun was allegedly wrested away from\n",
      "Pickering - who said he had another . After three-hour stand-off with\n",
      "police, he was found to be unarmed . Pickering has been charged with\n",
      "aggravated assault with a deadly weapon .<EOS>\n"
     ]
    }
   ],
   "source": [
    "# prints: [Example][<EOS>][<pad>][Example Summary][<EOS>]\n",
    "print(f'Single example:\\n\\n {detokenize(train_input)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T4sDS1WIVaYG"
   },
   "source": [
    "<a name='1.3'></a>\n",
    "\n",
    "## 1.3 Batching with bucketing\n",
    "\n",
    "As in the previous week, we use bucketing to create batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oqj1NsbERWQX"
   },
   "outputs": [],
   "source": [
    "# Bucketing to create batched generators.\n",
    "\n",
    "# Buckets are defined in terms of boundaries and batch sizes.\n",
    "# Batch_sizes[i] determines the batch size for items with length < boundaries[i]\n",
    "# So below, we'll take a batch of 16 sentences of length < 128 , 8 of length < 256,\n",
    "# 4 of length < 512. And so on. \n",
    "boundaries =  [128, 256,  512, 1024]\n",
    "batch_sizes = [16,    8,    4,    2, 1]\n",
    "\n",
    "# Create the streams.\n",
    "train_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries, batch_sizes)(train_stream)\n",
    "\n",
    "eval_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries, batch_sizes)(eval_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P6M5OA8QRWQb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1635)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Every execution will result in generation of a different article\n",
    "# Try running this cell multiple times to see how the length of the examples affects the batch size\n",
    "input_batch, _, mask_batch = next(train_batch_stream)\n",
    "\n",
    "# Shape of the input_batch\n",
    "input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "SjNOlljxTGuQ",
    "outputId": "9227c68c-6369-4ce8-8137-506c594f6ad2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  567   379  5942 15752     4   379  7226  5182  3047  6611   136  4601\n",
      "     3   137   180  1874 16958     4     2   406   560   429 11969 28081\n",
      "   379  9720 22449  3590  4601     3   137   180  2577 16958     4     2\n",
      "   406   560   429   379    27  1091    23  5736   527   213 24393 19142\n",
      "    16  1710   131  1353   793   131  1353 10662 15615   166   131    40\n",
      " 22956 21294    17   886  1248    36  2583   412    28 19919     3  3048\n",
      "  2276  8386     2   541     2  1353 25491 21217 16096    20  4140   132\n",
      "  2642   320   423    64 11406   131   186    68 16529  4502  1481    40\n",
      "    46  3630   320 24851    14    28  4578  7511   131  1353   411   213\n",
      " 17682   782   285   131   108   369  1151    28  1037     3   305  1353\n",
      " 11655  1560   320  1151   793    68 10662 17291   360    40    46  1768\n",
      "   691   163  6111   136   131    40 23062   186    46  2391  1019   412\n",
      "    28 19919     3  3048  2276  8386     2   541     2  3925 11133  7614\n",
      "  5894  7511   131  1353   381     3   312   131   721  1429  1019    28\n",
      "  4578  1248    68 16529  4502  1481   745  2585     2   286     2   131\n",
      "   233   213  1604    40   346    68 10662 15615   379  1333     2   412\n",
      "   131 11800   320  3606   750  1019  9783   242     2  2905  8386    23\n",
      "   133   213 21219   884   320  1566  2685    68 24393 19142     4   132\n",
      "   213  1525   285   103    39  1477    54 26977  1838 16168    16    68\n",
      "  9948     3   305   127  4601 27439  6050 13459  1628    13  1353   506\n",
      "   186    13   133    36 12451  2010  9948   285    13    39  1153  1019\n",
      "  8232 23707  6050 13459  1628   449   534  9948    39 13181     4   156\n",
      "  1019   213  1272   527   130   177   186    13   483    54   506   331\n",
      "   320   288   285  7511    41    18 22956 21294    17   886    41  1435\n",
      "  9631    16  1248    31   531   186    31 18037 23707  6050 13459  1628\n",
      "   861  9054    18    46  4099   166   527   579   285  2195   409    91\n",
      "  1008    35    13   483   320  1566  2685   103   320  1477   103  7147\n",
      "   320  1815  1788  4172 29725   391  2905  8386   742   131  1353   144\n",
      " 14598   216  7511   131  3557  3828   214  8149   171 19995  1248    28\n",
      "  2583  7511   131  1353   381     3   200   102   416  1019   163 15887\n",
      "     4  1166   131  1353   793   131    40 23062 11133  7614  5894     3\n",
      "   305  1353  2391  1248 20538     5   186   547   103   936    68     3\n",
      "   305   465  4601 27439  6050 13459  1628    13 29725     4    21  2401\n",
      "    28 23680    14 10877   809    28   506   277     2    35  2754   506\n",
      "  1091  1202 29725     4    26   191 14670  6995 27439  6050 13459  1628\n",
      "    13   547   103   936   156   186 16613    17   103    62   369  3001\n",
      "   457  4172 29725   391  2905  8386    40   945  7511   131  1353   381\n",
      "   186   742   131  1353  3825    17     3   312   131 13798   320 24851\n",
      "    14  4140  2441    68 23270 16938    47 16092    25  6364 12744  3095\n",
      "  7914   379   200  2905  8386  1353  2458  6278   285   213  2211    40\n",
      "   674    46   757     3    52  1793 29725     4    26   353   131  1326\n",
      "   745  2585   123    68   126   132    28   662   278     2   285   131\n",
      "  1353   320  6236   141  7270   196    68   177    40    46  2486   691\n",
      "    36  9948     3     9  1622  1746   345  1839   186   368  2585  1341\n",
      "     3   312    41   995    71    31   221   278   132 25093  9725  2299\n",
      "     2   415 10096     2   213  1622   721  1429  1019    28  4578     3\n",
      "   200   102    28   104  1248    92  8506  3048  2276   980    68 24933\n",
      "   132   508   420     3  2905  8386   186    68 16529  4502  1481  1435\n",
      "   169  1429   320  3606   213   750  1048   320  1153  1019  9783   242\n",
      "   945   379  9727  4140   408   179  1923    90   131  1353  2180   320\n",
      "    28  6419 18848 16741   958   132   426   420     3   305 16066   464\n",
      "    28   451   527 11810  3949   186   819     6 19422   132   560   420\n",
      "     2  1480  2441 12043  2140   132    68 16092     3  2905  8386   127\n",
      "  4601 27439  6050 13459  1628    13  1353 11655  1560  7511    41   793\n",
      "    93  2754    41    40   233     2    35  1353    19  2120   809    38\n",
      "  7511    41   127   103  1353  1069  1768   691 11133  7614  5894     3\n",
      "    13 12402 29725     4    26   188   793   745  2685   103   166   103\n",
      "    40  2195    90   209  1008     2    13   742   103  1353   132   213\n",
      "   543  4172 29725   391   200   169   103  1353  9071    68   531  9054\n",
      "   527 20091 15019  3487   186  2905  8386  1353   793   131  1353 10662\n",
      " 15615 23707  6050 13459  1628    13  4129   246   186 21153    21     3\n",
      "    13   141  3161 29725     4    26  1029   103     3    13  1353  9348\n",
      "  9924    21     2 14565    17   186 19081 23707  6050 13459  1628    13\n",
      "   742   166    13    40    46  2391    13    62  1151  1745    35   213\n",
      " 13035   127   213  2211    40   761   674    46   757     3    13    40\n",
      "    92  1004  4172 29725   391  1715    86  1525   527    28   228  2808\n",
      "   132  9783   242    35   412   368  2585    40    28  1330  1248   163\n",
      "  1819     6  2583   213  1622   124    19  9666  1019   945    78   213\n",
      " 25051     4     3   392   144 10047   213 10621  5067  2905  8386  4133\n",
      "  1248  6034     3   305   127  4601 27439  6050 13459  1628    13  1353\n",
      "   141  9635  2398  2883   374   194   186 25932    13   143   983   179\n",
      "   213  7528  4172 29725   391   392 25865 21885   113    68  1785   131\n",
      "   816  2685   163  1578     2  1480   143   399   320 10730  5914    68\n",
      " 16092   186   835    68 10842   527  1987  9682     3  1561 24933  3147\n",
      "    77  1353   141    28   271   318  1641  1832   103    62   126    35\n",
      "  1003   213   273  2504     3  2905  8386 16066   464   213  1578   132\n",
      "   568   824   104    35    23   234    19   424  9682     3  2905  8386\n",
      "  2976   320  3606  3866  2685   213 26024   527 22956 21294    17   886\n",
      "   412   131   465   131 16318   275    68   884   374   194   186  1202\n",
      "     7    26   483    54 26977   320   191   213   164  9948 11969 27439\n",
      "  6050 13459  1628   207  2232   320  8237 10730  5914   213   231  7243\n",
      "    35   213   346  1353   403 12438  7914  4617 29725   391   131  4846\n",
      " 23707  6050 13459  1628   198   229   234    28  7154  1832    13   439\n",
      "  1198  9682    90    13 14327   140   320   285    35    13   288  1577\n",
      "   246   213    86   608  1832   229   761  9783   242  4172 29725   391\n",
      "  2905  8386  1880   320   245   160   132   163  7882  3497  3233    35\n",
      "  1353  5976   166   527   213  6111   136     3   348   111  1687    66\n",
      "    88   226   186  1687    99    88   226   318  2562   213  1622  1435\n",
      "   169 21781   320  1153  1019  9783   242     3   305   465  4601 27439\n",
      "  6050 13459  1628   392   213  9932    77   229  7025  1298   346   320\n",
      "  2991    90    13 29725     4    75   981  2754    13    49   320  3606\n",
      "   213   750   176  1081  9677     4  1781     3  2905  8386    23    43\n",
      "   721    28  3004   843   320  2540    68  7763   320   412   125 17434\n",
      "   412   498     3  2905  8386   127  3611  3705   194    13 24868   213\n",
      "   884    13   133   412    28 19919    35    13   288    77  1435   761\n",
      "  3808   527    54 26977   981   213   164   186    13   483   320  2991\n",
      "   105   824 24393 19142     4 12893 27439  6050 13459  1628    13   362\n",
      "   103 29725     4     5 21809   285    13   358 29725     4    26  9666\n",
      "  1019   399    78   213 25051     4   166   130  2583   674    23    28\n",
      "   966     3   449 14251 29725     4    26  1477   156  1838   144    28\n",
      " 16111     3    13    62   107   320   172   285  1447  5772 23707  6050\n",
      " 13459  1628  3705   194    13 24868   213   884    13   133   412    28\n",
      " 19919    35    13   288    77  1435   761  3808   527    54 26977   981\n",
      "   213   164   186    13   483   320  2991   105   824 24393 19142     4\n",
      " 23707  6050 13459  1628    13   288    77  1435   101  1779    39  4038\n",
      "   156     2    35    51    38   191 14670   186 11060 16318   275   103\n",
      "    44    74   156 23707  6050 13459  1628    13  2657    13    40   369\n",
      "    40 22956 21294    17   886   186    13  2657    13    40   249   213\n",
      "   444  1980   527   213  2211   285    49  1151  1768 23707  6050 13459\n",
      "  1628    13 29725     4    75 23082    16  1019    28 23893     4   186\n",
      "  1525   285    36   194    13    39   211   213  1832   320  1151    28\n",
      "  1037 23707  6050 13459  1628    34   213 24426    13    39   124  2754\n",
      "    13    49   320   399    54   331   691  5475  3866  4172 29725   391\n",
      "   223   103   229    19  2391     2 11133  7614  5894    49  1275   209\n",
      "     6   632   912     3    34   331   103    49  2540   320   213  2281\n",
      " 13343     2 18697   283   186 23270 16938    47 16092    70   824    49\n",
      "  1275    28  1785   316  7394 21119 17967  1604     8  4125   136    24\n",
      "  4125   136    49  1275 10662 17291   360   186 18477  7394 21119  3987\n",
      "     3    52    43  3738   213   993   527 16526 25844   277   186 17761\n",
      "  5346     4  8149     3 17452  7614  5894    49    43  2540   320  1275\n",
      " 21892  8905   132   213 23270 16938    47 16092    70   824    49   191\n",
      "   103  1181  1019   163  7882   320  1536  1838   213 18697    20   320\n",
      "   213  2281 13343   454 14225    44  1181     3    34   313     2 11133\n",
      "  7614  5894    49  1275  5868 13214 24118  6067    70 15385    16   527\n",
      "   213   160   527   213   157     7     5  7531   238   285 10275 17327\n",
      "  1838   213 21229 18320     3   223   346 20859  9277    21     2   103\n",
      "    49   789   320 10662 17291   360    10     1     0  3048  2276  8386\n",
      "  3925 11133  7614  5894  1838    28  2583  7511   131  1353   381 16346\n",
      " 27439  6774  1628   305  1353  2391  1248 20538     5   186   742   131\n",
      "  1353  3825    17 16346 27439  6774  1628 25580     4  1019  4140  7511\n",
      "   131   186 16529  4502  1481   745  2585 13798   320 24851    14 16346\n",
      " 27439  6774  1628 16511     5  1606    68 23270 16938    47 16092    25\n",
      " 11054   412    28   581   527   213 11133  7614  5894 16346 27439  6774\n",
      "  1628     9  1622  1435   169 21781   320  1153  1019  9783   242   945\n",
      " 16346 27439  6774  1628   207  1435    19  3456   320  9783   242    78\n",
      "   213 25051     4   412   368  2585    23    28  1330  1838    28  1116\n",
      "  1018  2104     1]\n"
     ]
    }
   ],
   "source": [
    "# print corresponding integer values\n",
    "print(input_batch[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GD-72TENV2Jk"
   },
   "source": [
    "Things to notice:\n",
    " - First we see the corresponding values of the words.\n",
    " - The first 1, which represents the `<EOS>` tag of the article.\n",
    " - Followed by a 0, which represents a `<pad>` tag.\n",
    " - After the first 0 (`<pad>` tag) the corresponding values are of the words that are used for the summary of the article.\n",
    " - The second 1 represents the `<EOS>` tag for the summary.\n",
    " - All the trailing 0s represent `<pad>` tags which are appended to maintain consistent length (If you don't see them then it would mean it is already of max length)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Bu05ZwbWTE6P",
    "outputId": "3d455bd7-e343-4c25-a467-572d2abd837f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article:\n",
      "\n",
      " By . Kelly Strange . PUBLISHED: . 10:55 EST, 18 October 2013 . | .\n",
      "UPDATED: . 10:57 EST, 18 October 2013 . A woman has spoken of the\n",
      "heartbreaking moment she was told she was infertile because she had\n",
      "unprotected sex with one partner as a teenager. Jodie Watson, 21, was\n",
      "undergoing exploratory tests in hospital to find out why she and her\n",
      "fiancé had been unable to conceive a baby when she was given the\n",
      "devastating news that she may never be a mother. She was stunned to be\n",
      "told her infertility had been caused by an STD she had contracted and\n",
      "been treated for as a teenager. Jodie Watson, 21, caught chlamydia\n",
      "when she was 16. When she started trying for a baby with her fiancé\n",
      "James Jackson, 30, she found the disease had left her infertile . Now,\n",
      "as she struggles to raise money for IVF, Ms Watson has made the brave\n",
      "decision to talk about her heartbreak in the hope that it will stop\n",
      "other teenagers from repeating her mistake. She said: ‘I was young and\n",
      "I made one foolish mistake that I will pay for forever. ‘That single\n",
      "mistake will haunt me for the rest of my life and I want other young\n",
      "women to know that when they have unprotected sex they are dicing with\n",
      "their future and their fertility. ‘My dreams have been destroyed\n",
      "because of something that happened five years ago but I want to talk\n",
      "about it to stop it happening to anyone else.’ Ms Watson thought she\n",
      "was being sensible when she protected herself against pregnancy before\n",
      "sleeping with a partner when she was 16. But after going for an STI\n",
      "test she was told she had contracted chlamydia. She was treated with\n",
      "antibiotics and put it behind her. She says: ‘I’d learned a horrible\n",
      "lesson at a young age, but what young woman doesn’t make mistakes? ‘I\n",
      "put it behind me and vowed it would never happen again.’ Ms Watson had\n",
      "treatment when she was 16 and thought she was cured. When she\n",
      "struggled to conceive tests revealed her fallopian tubes were\n",
      "irreversibly damaged . But Ms Watson was unaware that the damage had\n",
      "already been done. It wasn’t until she met James Jackson through her\n",
      "work in a care home, that she was to discover just how much her life\n",
      "had been affected by one mistake. The couple quickly became serious\n",
      "and Mr Jackson proposed. When they moved into their own home in\n",
      "Hartlepool, County Durham, the couple started trying for a baby. But\n",
      "after a year with no joy Jodie saw her GP in April 2012. Ms Watson and\n",
      "her fiancé are now trying to raise the money needed to pay for IVF\n",
      "treatment . Blood tests came back normal so she was referred to a\n",
      "gynaecologist in June 2012. She underwent a series of ultrasounds and\n",
      "X-rays in October 2012, which revealed blockages in her tubes. Ms\n",
      "Watson said: ‘I was stunned when they told us what they had found, but\n",
      "was not prepared at all when they said it was usually caused by\n",
      "chlamydia. I hadn’t even told James about it because it had happened\n",
      "so long ago, I thought it was in the past.’ But now it was affecting\n",
      "her future dreams of motherhood and Ms Watson was told she was\n",
      "infertile. ‘I broke down and cried. I just couldn’t believe it. I was\n",
      "ashamed, embarrassed and shocked. ‘I thought because I had been\n",
      "treated I would be fine but the consultant said the damage had likely\n",
      "already been done. I had no idea.’ Their only hope of a family lies in\n",
      "IVF but as Mr Jackson had a daughter with an ex-partner the couple do\n",
      "not qualify for treatment on the NHS. After being dealt the bombshell\n",
      "Ms Watson suffered with depression. She said: ‘I was just blaming\n",
      "myself every day and wishing I could turn back the clock.’ After\n",
      "researching her condition she read about an operation, which could\n",
      "help to unblock her tubes and increase her chances of becoming\n",
      "pregnant. Her GP explained there was just a 15 per cent chance it\n",
      "would work but gave the go ahead. Ms Watson underwent the operation in\n",
      "January this year but has still not become pregnant. Ms Watson wants\n",
      "to raise awareness about the dangers of unprotected sex as she says\n",
      "she regrets her decision every day and doesn't want other teenagers to\n",
      "make the same mistake . ‘They managed to partially unblock the right\n",
      "tube but the left was too badly damaged,’ she explains. ‘There is\n",
      "still a tiny chance I might fall pregnant so I cling to that but I\n",
      "know deep down the only real chance is likely IVF.’ Ms Watson applied\n",
      "to take part in an egg sharing scheme but was rejected because of the\n",
      "STD. At between £4,000 and £6,000 per cycle the couple are now\n",
      "fundraising to pay for IVF. She says: ‘After the bills there is hardly\n",
      "anything left to save so I’m doing what I can to raise the money\n",
      "including car boot sales. Ms Watson has also started a Facebook page\n",
      "to spread her warning to as many teens as possible. Ms Watson said:\n",
      "'Every day I regret the decision I made as a teenager but I know there\n",
      "are likely hundreds of other teenagers doing the same and I want to\n",
      "save them this heartbreak' ‘I think it’s unfair that I don’t qualify\n",
      "for help on the NHS because my partner already has a child. That\n",
      "shouldn’t stop me from being a mum. I would like to see that rule\n",
      "reviewed. ‘Every day I regret the decision I made as a teenager but I\n",
      "know there are likely hundreds of other teenagers doing the same and I\n",
      "want to save them this heartbreak. ‘I know there are people who will\n",
      "judge me, but we all make mistakes and nobody regrets it more than me.\n",
      "‘I wish I had never had unprotected sex and I wish I had known the\n",
      "full extent of the damage that can be caused. ‘I’m praying for a\n",
      "miracle and hope that one day I will get the chance to be a mother.\n",
      "‘In the meantime I will do what I can to help other women by raising\n",
      "awareness.’ If it is not treated, chlamydia can cause long-term\n",
      "problems. In women it can spread to the womb, ovaries and fallopian\n",
      "tubes - this can cause a condition called pelvic inflammatory disease\n",
      "(PID). PID can cause infertility and persistent pelvic pain. It also\n",
      "increases the risk of miscarriage and ectopic pregnancy. Chlamydia can\n",
      "also spread to cause inflammation in the fallopian tubes - this can\n",
      "make it difficult for an egg to travel from the ovary to the womb\n",
      "making conception more difficult. In men, chlamydia can cause\n",
      "epididymitis - swelling of the part of the man's reproductive system\n",
      "that carries sperm from the testicles. If left untreated, it can lead\n",
      "to infertility.<EOS><pad>JodieWatson caught chlamydia from a partner\n",
      "when she was 16 . She was treated with antibiotics and thought she was\n",
      "cured . Went for tests when she and fiancé James Jackson struggled to\n",
      "conceive . Tests showed her fallopian tubes were blocked as a result\n",
      "of the chlamydia . The couple are now fundraising to pay for IVF\n",
      "treatment . They are not entitled to IVF on the NHS as Mr Jackson has\n",
      "a daughter from a previous relationship .<EOS>\n"
     ]
    }
   ],
   "source": [
    "# print the article and its summary\n",
    "print('Article:\\n\\n', detokenize(input_batch[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aNFVhgHoncGm"
   },
   "source": [
    "You can see that the data has the following structure:\n",
    "- <span style='color:blue'> [Article] </span> -> `<EOS>` -> `<pad>` -> <span style='color:blue'> [Article Summary] </span> -> `<EOS>` -> (possibly) multiple `<pad>`\n",
    "\n",
    "The loss is taken only on the summary using cross_entropy as loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Un8NHIRoj-1W"
   },
   "source": [
    "<a name='2'></a>\n",
    "# Part 2: Summarization with transformer\n",
    "\n",
    "Now that we have given you the data generator and have handled the preprocessing for you, it is time for you to build your own model. We saved you some time because we know you have already preprocessed data before in this specialization, so we would rather you spend your time doing the next steps. \n",
    "\n",
    "You will be implementing the attention from scratch and then using it in your transformer model. Concretely, you will understand how attention works, how you use it to connect the encoder and the decoder.\n",
    "\n",
    "<img src=\"transformer_decoder_zoomin.png\">\n",
    "\n",
    "<a name='2.1'></a>\n",
    "## 2.1 Dot product attention \n",
    "\n",
    "Now you will implement dot product attention which takes in a query, key, value, and a mask. It returns the output. \n",
    "\n",
    "<img src =\"dotproduct.png\">\n",
    "\n",
    "\n",
    "Here are some helper functions that will help you create tensors and display useful information:\n",
    "   - `create_tensor`  creates a `jax numpy array` from a list of lists.\n",
    "   - `display_tensor` prints out the shape and the actual tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensor(t):\n",
    "    \"\"\"Create tensor from list of lists\"\"\"\n",
    "    return jnp.array(t)\n",
    "\n",
    "\n",
    "def display_tensor(t, name):\n",
    "    \"\"\"Display shape and tensor\"\"\"\n",
    "    print(f'{name} shape: {t.shape}\\n')\n",
    "    print(f'{t}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before implementing it yourself, you can play around with a toy example of `dot product attention` without the softmax  operation. Technically it would not be `dot product attention` without the softmax but this is done to avoid giving away too much of the answer and the idea is to display these tensors to give you a sense of how they look like.\n",
    "\n",
    "The formula for attention is this one:\n",
    "\n",
    "$$\n",
    "\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\tag{1}\\\n",
    "$$\n",
    "\n",
    "$d_{k}$ stands for the dimension of queries and keys.\n",
    "\n",
    "The `query`, `key`, `value` and `mask` vectors are provided for this example.\n",
    "\n",
    "Notice that the masking is done using very negative values that will yield a similar effect to using $-\\infty $. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "_0x0HJXwRWQk",
    "outputId": "d6d78a8e-e3cc-47af-9584-2bdcdfcca0cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query shape: (2, 3)\n",
      "\n",
      "[[1 0 0]\n",
      " [0 1 0]]\n",
      "\n",
      "key shape: (2, 3)\n",
      "\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "\n",
      "value shape: (2, 3)\n",
      "\n",
      "[[0 1 0]\n",
      " [1 0 1]]\n",
      "\n",
      "mask shape: (2, 2)\n",
      "\n",
      "[[ 0.e+00  0.e+00]\n",
      " [-1.e+09  0.e+00]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = create_tensor([[1, 0, 0], [0, 1, 0]])\n",
    "display_tensor(q, 'query')\n",
    "k = create_tensor([[1, 2, 3], [4, 5, 6]])\n",
    "display_tensor(k, 'key')\n",
    "v = create_tensor([[0, 1, 0], [1, 0, 1]])\n",
    "display_tensor(v, 'value')\n",
    "m = create_tensor([[0, 0], [-1e9, 0]])\n",
    "display_tensor(m, 'mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    "query shape: (2, 3)\n",
    "\n",
    "[[1 0 0]\n",
    " [0 1 0]]\n",
    "\n",
    "key shape: (2, 3)\n",
    "\n",
    "[[1 2 3]\n",
    " [4 5 6]]\n",
    "\n",
    "value shape: (2, 3)\n",
    "\n",
    "[[0 1 0]\n",
    " [1 0 1]]\n",
    "\n",
    "mask shape: (2, 2)\n",
    "\n",
    "[[ 0.e+00  0.e+00]\n",
    " [-1.e+09  0.e+00]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "kVR9u4faRWQo",
    "outputId": "f01ea4ca-4152-4b54-b76a-e4b5917ae2b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query dot key shape: (2, 2)\n",
      "\n",
      "[[0.57735026 2.309401  ]\n",
      " [1.1547005  2.8867514 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_dot_k = q @ k.T / jnp.sqrt(3)\n",
    "display_tensor(q_dot_k, 'query dot key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    "query dot key shape: (2, 2)\n",
    "\n",
    "[[0.57735026 2.309401  ]\n",
    " [1.1547005  2.8867514 ]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked query dot key shape: (2, 2)\n",
      "\n",
      "[[ 5.7735026e-01  2.3094010e+00]\n",
      " [-1.0000000e+09  2.8867514e+00]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "masked = q_dot_k + m\n",
    "display_tensor(masked, 'masked query dot key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    "masked query dot key shape: (2, 2)\n",
    "\n",
    "[[ 5.7735026e-01  2.3094010e+00]\n",
    " [-1.0000000e+09  2.8867514e+00]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked query dot key dot value shape: (2, 3)\n",
      "\n",
      "[[ 2.3094010e+00  5.7735026e-01  2.3094010e+00]\n",
      " [ 2.8867514e+00 -1.0000000e+09  2.8867514e+00]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_tensor(masked @ v, 'masked query dot key dot value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    "masked query dot key dot value shape: (2, 3)\n",
    "\n",
    "[[ 2.3094010e+00  5.7735026e-01  2.3094010e+00]\n",
    " [ 2.8867514e+00 -1.0000000e+09  2.8867514e+00]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the previous dummy tensors to test some of the graded functions, a batch dimension should be added to them so they mimic the shape of real-life examples. The mask is also replaced by a version of it that resembles the one that is used by trax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query with batch dim shape: (1, 2, 3)\n",
      "\n",
      "[[[1 0 0]\n",
      "  [0 1 0]]]\n",
      "\n",
      "key with batch dim shape: (1, 2, 3)\n",
      "\n",
      "[[[1 2 3]\n",
      "  [4 5 6]]]\n",
      "\n",
      "value with batch dim shape: (1, 2, 3)\n",
      "\n",
      "[[[0 1 0]\n",
      "  [1 0 1]]]\n",
      "\n",
      "boolean mask shape: (2, 2)\n",
      "\n",
      "[[ True  True]\n",
      " [False  True]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_with_batch = q[None,:]\n",
    "display_tensor(q_with_batch, 'query with batch dim')\n",
    "k_with_batch = k[None,:]\n",
    "display_tensor(k_with_batch, 'key with batch dim')\n",
    "v_with_batch = v[None,:]\n",
    "display_tensor(v_with_batch, 'value with batch dim')\n",
    "m_bool = create_tensor([[True, True], [False, True]])\n",
    "display_tensor(m_bool, 'boolean mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    "query with batch dim shape: (1, 2, 3)\n",
    "\n",
    "[[[1 0 0]\n",
    "  [0 1 0]]]\n",
    "\n",
    "key with batch dim shape: (1, 2, 3)\n",
    "\n",
    "[[[1 2 3]\n",
    "  [4 5 6]]]\n",
    "\n",
    "value with batch dim shape: (1, 2, 3)\n",
    "\n",
    "[[[0 1 0]\n",
    "  [1 0 1]]]\n",
    "\n",
    "boolean mask shape: (2, 2)\n",
    "\n",
    "[[ True  True]\n",
    " [False  True]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex01'></a>\n",
    "### Exercise 01\n",
    "\n",
    "**Instructions:** Implement the dot product attention. Concretely, implement the following equation\n",
    "\n",
    "\n",
    "$$\n",
    "\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\tag{1}\\\n",
    "$$\n",
    "\n",
    "$Q$ - query, \n",
    "$K$ - key, \n",
    "$V$ - values, \n",
    "$M$ - mask, \n",
    "${d_k}$ - depth/dimension of the queries and keys (used for scaling down)\n",
    "\n",
    "You can implement this formula either by `trax` numpy (trax.math.numpy) or regular `numpy` but it is recommended to use `jnp`.\n",
    "\n",
    "Something to take into consideration is that within trax, the masks are tensors of `True/False` values not 0's and $-\\infty$ as in the previous example. Within the graded function don't think of applying the mask by summing up matrices, instead use `jnp.where()` and treat the **mask as a tensor of boolean values with `False` for values that need to be masked and True for the ones that don't.**\n",
    "\n",
    "Also take into account that the real tensors are far more complex than the toy ones you just played with. Because of this avoid using shortened operations such as `@` for dot product or `.T` for transposing. Use `jnp.matmul()` and `jnp.swapaxes()` instead.\n",
    "\n",
    "This is the self-attention block for the transformer decoder. Good luck!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kSauPt0NUl_o"
   },
   "outputs": [],
   "source": [
    "# UNQ_C1\n",
    "# GRADED FUNCTION: DotProductAttention\n",
    "def DotProductAttention(query, key, value, mask):\n",
    "    \"\"\"Dot product self-attention.\n",
    "    Args:\n",
    "        query (jax.interpreters.xla.DeviceArray): array of query representations with shape (L_q by d)\n",
    "        key (jax.interpreters.xla.DeviceArray): array of key representations with shape (L_k by d)\n",
    "        value (jax.interpreters.xla.DeviceArray): array of value representations with shape (L_k by d) where L_v = L_k\n",
    "        mask (jax.interpreters.xla.DeviceArray): attention-mask, gates attention with shape (L_q by L_k)\n",
    "\n",
    "    Returns:\n",
    "        jax.interpreters.xla.DeviceArray: Self-attention array for q, k, v arrays. (L_q by L_k)\n",
    "    \"\"\"\n",
    "\n",
    "    assert query.shape[-1] == key.shape[-1] == value.shape[-1], \"Embedding dimensions of q, k, v aren't all the same\"\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    # Save depth/dimension of the query embedding for scaling down the dot product\n",
    "    depth = query.shape[-1] \n",
    "\n",
    "    # Calculate scaled query key dot product according to formula above\n",
    "    dots = jnp.matmul(query, jnp.swapaxes(key, -1, -2)) / jnp.sqrt(depth)\n",
    "\n",
    "    # Apply the mask\n",
    "    if mask is not None: # The 'None' in this line does not need to be replaced\n",
    "        dots = jnp.where(mask, dots, jnp.full_like(dots, -1e9))\n",
    "    \n",
    "    # Softmax formula implementation\n",
    "    # Use trax.fastmath.logsumexp of dots to avoid underflow by division by large numbers\n",
    "    # Hint: Last axis should be used and keepdims should be True\n",
    "    # Note: softmax = e^(dots - logsumexp(dots)) = E^dots / sumexp(dots)\n",
    "    logsumexp = trax.fastmath.logsumexp(dots, axis=-1, keepdims=True)\n",
    "\n",
    "    # Take exponential of dots minus logsumexp to get softmax\n",
    "    # Use jnp.exp()\n",
    "    dots = jnp.exp(dots - logsumexp)\n",
    "\n",
    "    # Multiply dots by value to get self-attention\n",
    "    # Use jnp.matmul()\n",
    "    attention = jnp.matmul(dots, value)\n",
    "\n",
    "    ## END CODE HERE ###\n",
    "    \n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "8o0K7VWKRWQw",
    "outputId": "1c51af3a-5f11-480f-b33b-419072d8298c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[[0.8496746 , 0.15032545, 0.8496746 ],\n",
       "              [1.        , 0.        , 1.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DotProductAttention(q_with_batch, k_with_batch, v_with_batch, m_bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    "DeviceArray([[[0.8496746 , 0.15032545, 0.8496746 ],\n",
    "              [1.        , 0.        , 1.        ]]], dtype=float32)\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2y2PSiLVRWQ2"
   },
   "source": [
    "<a name='2.2'></a>\n",
    "\n",
    "## 2.2 Causal Attention\n",
    "\n",
    "Now you are going to implement causal attention: multi-headed attention with a mask to attend only to words that occurred before. \n",
    "\n",
    "<img src = \"causal.png\">\n",
    "\n",
    "In the image above, a word can see everything that is before it, but not what is after it. To implement causal attention, you will have to transform vectors and do many reshapes. You will need to implement the functions below.\n",
    "\n",
    "\n",
    "<a name='ex02'></a>\n",
    "### Exercise 02\n",
    "\n",
    "Implement the following functions that will be needed for Causal Attention:\n",
    "\n",
    "- <span style='color:blue'> compute_attention_heads </span>: Gets an input $x$ of dimension (batch_size, seqlen, n_heads $\\times$ d_head) and splits the last (depth) dimension and stacks it to the zeroth dimension to allow matrix multiplication (batch_size $\\times$ n_heads, seqlen, d_head).\n",
    "- <span style='color:blue'> dot_product_self_attention </span>: Creates a mask matrix with `False` values above the diagonal and `True` values below and calls DotProductAttention which implements dot product self attention.\n",
    "- <span style='color:blue'> compute_attention_output </span>: Undoes compute_attention_heads by splitting first (vertical) dimension and stacking in the last (depth) dimension (batch_size, seqlen, n_heads $\\times$ d_head). These operations concatenate (stack/merge) the heads. \n",
    "\n",
    "Next there are some toy tensors which may serve to give you an idea of the data shapes and opperations involved in Causal Attention. They are also useful to test out your functions! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "VRH67YcrRWQ3",
    "outputId": "847a9416-877a-4246-c738-0eacdf46de59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query matrix (2D tensor) shape: (2, 3)\n",
      "\n",
      "[[1 0 0]\n",
      " [0 1 0]]\n",
      "\n",
      "batch of two (multi-head) collections of query matrices (4D tensor) shape: (2, 2, 2, 3)\n",
      "\n",
      "[[[[1 0 0]\n",
      "   [0 1 0]]\n",
      "\n",
      "  [[1 0 0]\n",
      "   [0 1 0]]]\n",
      "\n",
      "\n",
      " [[[1 0 0]\n",
      "   [0 1 0]]\n",
      "\n",
      "  [[1 0 0]\n",
      "   [0 1 0]]]]\n",
      "\n",
      "one batch of concatenated heads of query matrices (3d tensor) shape: (1, 2, 6)\n",
      "\n",
      "[[[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]]\n",
      "\n",
      "three batches of concatenated heads of query matrices (3d tensor) shape: (3, 2, 6)\n",
      "\n",
      "[[[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]\n",
      "\n",
      " [[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]\n",
      "\n",
      " [[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tensor2d = create_tensor(q)\n",
    "display_tensor(tensor2d, 'query matrix (2D tensor)')\n",
    "\n",
    "tensor4d2b = create_tensor([[q, q], [q, q]])\n",
    "display_tensor(tensor4d2b, 'batch of two (multi-head) collections of query matrices (4D tensor)')\n",
    "\n",
    "tensor3dc = create_tensor([jnp.concatenate([q, q], axis = -1)])\n",
    "display_tensor(tensor3dc, 'one batch of concatenated heads of query matrices (3d tensor)')\n",
    "\n",
    "tensor3dc3b = create_tensor([jnp.concatenate([q, q], axis = -1), jnp.concatenate([q, q], axis = -1), jnp.concatenate([q, q], axis = -1)])\n",
    "display_tensor(tensor3dc3b, 'three batches of concatenated heads of query matrices (3d tensor)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to know that the following 3 functions would normally be defined within the `CausalAttention` function further below. \n",
    "\n",
    "However this makes these functions harder to test. Because of this, these functions are shown individually using a `closure` (when necessary) that simulates them being inside of the `CausalAttention` function. This is done because they rely on some variables that can be accessed from within `CausalAttention`.\n",
    "\n",
    "### Support Functions\n",
    "\n",
    "<span style='color:blue'> compute_attention_heads </span>: Gets an input $x$ of dimension (batch_size, seqlen, n_heads $\\times$ d_head) and splits the last (depth) dimension and stacks it to the zeroth dimension to allow matrix multiplication (batch_size $\\times$ n_heads, seqlen, d_head).\n",
    "\n",
    "**For the closures you only have to fill the inner function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2\n",
    "# GRADED FUNCTION: compute_attention_heads_closure\n",
    "def compute_attention_heads_closure(n_heads, d_head):\n",
    "    \"\"\" Function that simulates environment inside CausalAttention function.\n",
    "    Args:\n",
    "        d_head (int):  dimensionality of heads.\n",
    "        n_heads (int): number of attention heads.\n",
    "    Returns:\n",
    "        function: compute_attention_heads function\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_attention_heads(x):\n",
    "        \"\"\" Compute the attention heads.\n",
    "        Args:\n",
    "            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size, seqlen, n_heads X d_head).\n",
    "        Returns:\n",
    "            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size X n_heads, seqlen, d_head).\n",
    "        \"\"\"\n",
    "        ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "        \n",
    "        # Size of the x's batch dimension\n",
    "        batch_size = x.shape[0]\n",
    "        # Length of the sequence\n",
    "        # Should be size of x's first dimension without counting the batch dim\n",
    "        seqlen = x.shape[1]\n",
    "        \n",
    "        \n",
    "        # Reshape x using jnp.reshape()\n",
    "        # batch_size, seqlen, n_heads*d_head -> batch_size, seqlen, n_heads, d_head\n",
    "        x = jnp.reshape(x, (batch_size, seqlen, n_heads, d_head))\n",
    "        \n",
    "        \n",
    "        # Transpose x using jnp.transpose()\n",
    "        # batch_size, seqlen, n_heads, d_head -> batch_size, n_heads, seqlen, d_head\n",
    "        # Note that the values within the tuple are the indexes of the dimensions of x and you must rearrange them\n",
    "        x = jnp.transpose(x, (0, 2, 1, 3))\n",
    "        \n",
    "        \n",
    "        # Reshape x using jnp.reshape()\n",
    "        # batch_size, n_heads, seqlen, d_head -> batch_size*n_heads, seqlen, d_head\n",
    "        x = jnp.reshape(x, (-1, seqlen, d_head))\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    return compute_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor shape: (3, 2, 6)\n",
      "\n",
      "[[[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]\n",
      "\n",
      " [[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]\n",
      "\n",
      " [[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]]\n",
      "\n",
      "output tensor shape: (6, 2, 3)\n",
      "\n",
      "[[[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_tensor(tensor3dc3b, \"input tensor\")\n",
    "result_cah = compute_attention_heads_closure(2,3)(tensor3dc3b)\n",
    "display_tensor(result_cah, \"output tensor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    "input tensor shape: (3, 2, 6)\n",
    "\n",
    "[[[1 0 0 1 0 0]\n",
    "  [0 1 0 0 1 0]]\n",
    "\n",
    " [[1 0 0 1 0 0]\n",
    "  [0 1 0 0 1 0]]\n",
    "\n",
    " [[1 0 0 1 0 0]\n",
    "  [0 1 0 0 1 0]]]\n",
    "\n",
    "output tensor shape: (6, 2, 3)\n",
    "\n",
    "[[[1 0 0]\n",
    "  [0 1 0]]\n",
    "\n",
    " [[1 0 0]\n",
    "  [0 1 0]]\n",
    "\n",
    " [[1 0 0]\n",
    "  [0 1 0]]\n",
    "\n",
    " [[1 0 0]\n",
    "  [0 1 0]]\n",
    "\n",
    " [[1 0 0]\n",
    "  [0 1 0]]\n",
    "\n",
    " [[1 0 0]\n",
    "  [0 1 0]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:blue'> dot_product_self_attention </span>: Creates a mask matrix with `False` values above the diagonal and `True` values below and calls DotProductAttention which implements dot product self attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3\n",
    "# GRADED FUNCTION: dot_product_self_attention\n",
    "def dot_product_self_attention(q, k, v):\n",
    "    \"\"\" Masked dot product self attention.\n",
    "    Args:\n",
    "        q (jax.interpreters.xla.DeviceArray): queries.\n",
    "        k (jax.interpreters.xla.DeviceArray): keys.\n",
    "        v (jax.interpreters.xla.DeviceArray): values.\n",
    "    Returns:\n",
    "        jax.interpreters.xla.DeviceArray: masked dot product self attention tensor.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # Hint: mask size should be equal to L_q. Remember that q has shape (batch_size, L_q, d)\n",
    "    # NOTE: there is a revision underway with the autograder to tolerate better indexing. \n",
    "    # Until then, please index q.shape using negative values (this is equivalent to counting from right to left)\n",
    "    mask_size = q.shape[-2]\n",
    "\n",
    "    # Creates a matrix with ones below the diagonal and 0s above. It should have shape (1, mask_size, mask_size)\n",
    "    # Notice that 1's and 0's get casted to True/False by setting dtype to jnp.bool_\n",
    "    # Use jnp.tril() - Lower triangle of an array and jnp.ones()\n",
    "    mask = jnp.tril(jnp.ones((1, mask_size, mask_size), dtype=jnp.bool_), k=0)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return DotProductAttention(q, k, v, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[[0.        , 1.        , 0.        ],\n",
       "              [0.8496746 , 0.15032543, 0.8496746 ]]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_product_self_attention(q_with_batch, k_with_batch, v_with_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    "DeviceArray([[[0.        , 1.        , 0.        ],\n",
    "              [0.8496746 , 0.15032543, 0.8496746 ]]], dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:blue'> compute_attention_output </span>: Undoes compute_attention_heads by splitting first (vertical) dimension and stacking in the last (depth) dimension (batch_size, seqlen, n_heads $\\times$ d_head). These operations concatenate (stack/merge) the heads. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4\n",
    "# GRADED FUNCTION: compute_attention_output_closure\n",
    "def compute_attention_output_closure(n_heads, d_head):\n",
    "    \"\"\" Function that simulates environment inside CausalAttention function.\n",
    "    Args:\n",
    "        d_head (int):  dimensionality of heads.\n",
    "        n_heads (int): number of attention heads.\n",
    "    Returns:\n",
    "        function: compute_attention_output function\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute_attention_output(x):\n",
    "        \"\"\" Compute the attention output.\n",
    "        Args:\n",
    "            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size X n_heads, seqlen, d_head).\n",
    "        Returns:\n",
    "            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size, seqlen, n_heads X d_head).\n",
    "        \"\"\"\n",
    "        ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "        \n",
    "        # Length of the sequence\n",
    "        # Should be size of x's first dimension without counting the batch dim\n",
    "        seqlen = x.shape[1]\n",
    "        \n",
    "        # Reshape x using jnp.reshape() to shape (batch_size, n_heads, seqlen, d_head)\n",
    "        x = jnp.reshape(x, ( -1, n_heads, seqlen, d_head))\n",
    "\n",
    "        # Transpose x using jnp.transpose() to shape (batch_size, seqlen, n_heads, d_head)\n",
    "        x = jnp.transpose(x, ( 0, 2, 1 , 3))\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Reshape to allow to concatenate the heads\n",
    "        return jnp.reshape(x, (-1, seqlen, n_heads * d_head))\n",
    "    \n",
    "    return compute_attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor shape: (6, 2, 3)\n",
      "\n",
      "[[[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]]\n",
      "\n",
      "output tensor shape: (3, 2, 6)\n",
      "\n",
      "[[[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]\n",
      "\n",
      " [[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]\n",
      "\n",
      " [[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_tensor(result_cah, \"input tensor\")\n",
    "result_cao = compute_attention_output_closure(2,3)(result_cah)\n",
    "display_tensor(result_cao, \"output tensor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    "input tensor shape: (6, 2, 3)\n",
    "\n",
    "[[[1 0 0]\n",
    "  [0 1 0]]\n",
    "\n",
    " [[1 0 0]\n",
    "  [0 1 0]]\n",
    "\n",
    " [[1 0 0]\n",
    "  [0 1 0]]\n",
    "\n",
    " [[1 0 0]\n",
    "  [0 1 0]]\n",
    "\n",
    " [[1 0 0]\n",
    "  [0 1 0]]\n",
    "\n",
    " [[1 0 0]\n",
    "  [0 1 0]]]\n",
    "\n",
    "output tensor shape: (3, 2, 6)\n",
    "\n",
    "[[[1 0 0 1 0 0]\n",
    "  [0 1 0 0 1 0]]\n",
    "\n",
    " [[1 0 0 1 0 0]\n",
    "  [0 1 0 0 1 0]]\n",
    "\n",
    " [[1 0 0 1 0 0]\n",
    "  [0 1 0 0 1 0]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Attention Function\n",
    "\n",
    "Now it is time for you to put everything together within the `CausalAttention` or Masked multi-head attention function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"masked-attention.png\"> \n",
    "\n",
    "**Instructions:** Implement the causal attention.\n",
    "Your model returns the causal attention through a $tl.Serial$ with the following:\n",
    "\n",
    "- <span style='color:blue'> [tl.Branch](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Branch) </span>: consisting of 3 [tl.Dense(d_feature), ComputeAttentionHeads] to account for the queries, keys, and values.\n",
    "- <span style='color:blue'> [tl.Fn](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn)</span>: Takes in dot_product_self_attention function and uses it to compute the dot product using $Q$, $K$, $V$.\n",
    "- <span style='color:blue'> [tl.Fn](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn)</span>: Takes in restack_attention_heads to allow for parallel computing.\n",
    "- <span style='color:blue'> [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense)</span>: Final Dense layer, with dimension `d_feature`.\n",
    "\n",
    "Remember that in order for trax to properly handle the functions you just defined, they need to be added as layers using the [`tl.Fn()`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn) function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B9Adn6DtRWRG"
   },
   "outputs": [],
   "source": [
    "# UNQ_C5\n",
    "# GRADED FUNCTION: CausalAttention\n",
    "def CausalAttention(d_feature, \n",
    "                    n_heads, \n",
    "                    compute_attention_heads_closure=compute_attention_heads_closure,\n",
    "                    dot_product_self_attention=dot_product_self_attention,\n",
    "                    compute_attention_output_closure=compute_attention_output_closure,\n",
    "                    mode='train'):\n",
    "    \"\"\"Transformer-style multi-headed causal attention.\n",
    "\n",
    "    Args:\n",
    "        d_feature (int):  dimensionality of feature embedding.\n",
    "        n_heads (int): number of attention heads.\n",
    "        compute_attention_heads_closure (function): Closure around compute_attention heads.\n",
    "        dot_product_self_attention (function): dot_product_self_attention function. \n",
    "        compute_attention_output_closure (function): Closure around compute_attention_output. \n",
    "        mode (str): 'train' or 'eval'.\n",
    "\n",
    "    Returns:\n",
    "        trax.layers.combinators.Serial: Multi-headed self-attention model.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert d_feature % n_heads == 0\n",
    "    d_head = d_feature // n_heads\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # HINT: The second argument to tl.Fn() is an uncalled function (without the parentheses)\n",
    "    # Since you are dealing with closures you might need to call the outer \n",
    "    # function with the correct parameters to get the actual uncalled function.\n",
    "    ComputeAttentionHeads = tl.Fn('AttnHeads', compute_attention_heads_closure(n_heads, d_head), n_out=1)\n",
    "    \n",
    "    \n",
    "    return tl.Serial(\n",
    "        tl.Branch( # creates three towers for one input, takes activations and creates queries keys and values\n",
    "            [tl.Dense(d_feature), ComputeAttentionHeads], # queries\n",
    "            [tl.Dense(d_feature), ComputeAttentionHeads], # keys\n",
    "            [tl.Dense(d_feature), ComputeAttentionHeads], # values\n",
    "        ),\n",
    "        \n",
    "        tl.Fn('DotProductAttn', dot_product_self_attention, n_out=1), # takes QKV\n",
    "        # HINT: The second argument to tl.Fn() is an uncalled function\n",
    "        # Since you are dealing with closures you might need to call the outer \n",
    "        # function with the correct parameters to get the actual uncalled function.\n",
    "        tl.Fn('AttnOutput', compute_attention_output_closure(n_heads, d_head), n_out=1), # to allow for parallel\n",
    "        tl.Dense(d_feature) # Final dense layer\n",
    "    )\n",
    "\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial[\n",
      "  Branch_out3[\n",
      "    [Dense_512, AttnHeads]\n",
      "    [Dense_512, AttnHeads]\n",
      "    [Dense_512, AttnHeads]\n",
      "  ]\n",
      "  DotProductAttn_in3\n",
      "  AttnOutput\n",
      "  Dense_512\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the causal attention model\n",
    "print(CausalAttention(d_feature=512, n_heads=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    "Serial[\n",
    "  Branch_out3[\n",
    "    [Dense_512, AttnHeads]\n",
    "    [Dense_512, AttnHeads]\n",
    "    [Dense_512, AttnHeads]\n",
    "  ]\n",
    "  DotProductAttn_in3\n",
    "  AttnOutput\n",
    "  Dense_512\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W6zwtPjqRWRJ"
   },
   "source": [
    "<a name='2.3'></a>\n",
    "\n",
    "## 2.3 Transformer decoder block\n",
    "\n",
    "Now that you have implemented the causal part of the transformer, you will implement the transformer decoder block. Concretely you will be implementing this image now.\n",
    "\n",
    "<img src = \"transformer_decoder_1.png\" style = \"height:300px\"> \n",
    "\n",
    "To implement this function, you will have to call the `CausalAttention` or Masked multi-head attention function you implemented above. You will have to add a feedforward which consists of: \n",
    "\n",
    "- <span style='color:blue'> [tl.LayerNorm](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm) </span>: used to layer normalize\n",
    "- <span style='color:blue'> [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense) </span>: the dense layer\n",
    "- <span style='color:blue'> [ff_activation](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.activation_fns.Relu) </span>: feed forward activation (we use ReLu) here.\n",
    "- <span style='color:blue'> [tl.Dropout](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout) </span>: dropout layer\n",
    "- <span style='color:blue'> [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense) </span>: dense layer\n",
    "- <span style='color:blue'> [tl.Dropout](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout) </span>: dropout layer\n",
    "\n",
    "Finally once you implement the feedforward, you can go ahead and implement the entire block using: \n",
    "\n",
    "- <span style='color:blue'> [tl.Residual](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Residual) </span>: takes in the tl.LayerNorm(), causal attention block, tl.dropout. \n",
    "\n",
    "- <span style='color:blue'> [tl.Residual](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Residual) </span>: takes in the feedforward block you will implement. \n",
    "\n",
    "<a name='ex03'></a>\n",
    "### Exercise 03\n",
    "**Instructions:** Implement the transformer decoder block. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKOxnRbp1K5U"
   },
   "outputs": [],
   "source": [
    "# UNQ_C6\n",
    "# GRADED FUNCTION: DecoderBlock\n",
    "def DecoderBlock(d_model, d_ff, n_heads,\n",
    "                 dropout, mode, ff_activation):\n",
    "    \"\"\"Returns a list of layers that implements a Transformer decoder block.\n",
    "\n",
    "    The input is an activation tensor.\n",
    "\n",
    "    Args:\n",
    "        d_model (int):  depth of embedding.\n",
    "        d_ff (int): depth of feed-forward layer.\n",
    "        n_heads (int): number of attention heads.\n",
    "        dropout (float): dropout rate (how much to drop out).\n",
    "        mode (str): 'train' or 'eval'.\n",
    "        ff_activation (function): the non-linearity in feed-forward layer.\n",
    "\n",
    "    Returns:\n",
    "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # Create masked multi-head attention block using CausalAttention function\n",
    "    causal_attention = CausalAttention( \n",
    "                        d_model,\n",
    "                        n_heads=n_heads,\n",
    "                        mode=mode\n",
    "                        )\n",
    "\n",
    "    # Create feed-forward block (list) with two dense layers with dropout and input normalized\n",
    "    feed_forward = [ \n",
    "        # Normalize layer inputs\n",
    "        tl.LayerNorm(),\n",
    "        # Add first feed forward (dense) layer (don't forget to set the correct value for n_units)\n",
    "        tl.Dense(d_ff),\n",
    "        # Add activation function passed in as a parameter (you need to call it!)\n",
    "        ff_activation(), # Generally ReLU\n",
    "        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n",
    "        tl.Dropout(rate=dropout, mode=mode),\n",
    "        # Add second feed forward layer (don't forget to set the correct value for n_units)\n",
    "        tl.Dense(d_model),\n",
    "        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n",
    "        tl.Dropout(rate=dropout,mode=mode)\n",
    "    ]\n",
    "\n",
    "    # Add list of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\n",
    "    return [\n",
    "      tl.Residual(\n",
    "          # Normalize layer input\n",
    "          tl.LayerNorm(),\n",
    "          # Add causal attention block previously defined (without parentheses)\n",
    "          causal_attention,\n",
    "          # Add dropout with rate and mode specified\n",
    "          tl.Dropout(rate=dropout, mode=mode)\n",
    "        ),\n",
    "      tl.Residual(\n",
    "          # Add feed forward block (without parentheses)\n",
    "          feed_forward\n",
    "        ),\n",
    "      ]\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Serial[\n",
      "  Branch_out2[\n",
      "    None\n",
      "    Serial[\n",
      "      LayerNorm\n",
      "      Serial[\n",
      "        Branch_out3[\n",
      "          [Dense_512, AttnHeads]\n",
      "          [Dense_512, AttnHeads]\n",
      "          [Dense_512, AttnHeads]\n",
      "        ]\n",
      "        DotProductAttn_in3\n",
      "        AttnOutput\n",
      "        Dense_512\n",
      "      ]\n",
      "      Dropout\n",
      "    ]\n",
      "  ]\n",
      "  Add_in2\n",
      "], Serial[\n",
      "  Branch_out2[\n",
      "    None\n",
      "    Serial[\n",
      "      LayerNorm\n",
      "      Dense_2048\n",
      "      Relu\n",
      "      Dropout\n",
      "      Dense_512\n",
      "      Dropout\n",
      "    ]\n",
      "  ]\n",
      "  Add_in2\n",
      "]]\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the decoder block\n",
    "print(DecoderBlock(d_model=512, d_ff=2048, n_heads=8, dropout=0.1, mode='train', ff_activation=tl.Relu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    "[Serial[\n",
    "  Branch_out2[\n",
    "    None\n",
    "    Serial[\n",
    "      LayerNorm\n",
    "      Serial[\n",
    "        Branch_out3[\n",
    "          [Dense_512, AttnHeads]\n",
    "          [Dense_512, AttnHeads]\n",
    "          [Dense_512, AttnHeads]\n",
    "        ]\n",
    "        DotProductAttn_in3\n",
    "        AttnOutput\n",
    "        Dense_512\n",
    "      ]\n",
    "      Dropout\n",
    "    ]\n",
    "  ]\n",
    "  Add_in2\n",
    "], Serial[\n",
    "  Branch_out2[\n",
    "    None\n",
    "    Serial[\n",
    "      LayerNorm\n",
    "      Dense_2048\n",
    "      Relu\n",
    "      Dropout\n",
    "      Dense_512\n",
    "      Dropout\n",
    "    ]\n",
    "  ]\n",
    "  Add_in2\n",
    "]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SoFv-nfLRWRN",
    "lines_to_next_cell": 0
   },
   "source": [
    "<a name='2.4'></a>\n",
    "## 2.4 Transformer Language Model\n",
    "\n",
    "You will now bring it all together. In this part you will use all the subcomponents you previously built to make the final model. Concretely, here is the image you will be implementing. \n",
    "<img src = \"transformer_decoder.png\" style = \"height:400px\">\n",
    "\n",
    "    \n",
    "<a name='ex04'></a>\n",
    "### Exercise 04\n",
    "**Instructions:** Previously you coded the decoder block. Now you will code the transformer language model. Here is what you will need. \n",
    "\n",
    "- <span style=\"color:blue\"> positional_enconder </span>- a list containing the following layers:\n",
    "    - <span style=\"color:blue\"> [tl.Embedding](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding)\n",
    "    - <span style=\"color:blue\"> [tl.Dropout](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout)\n",
    "    - <span style=\"color:blue\"> [tl.PositionalEncoding](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.PositionalEncoding)\n",
    "\n",
    "- A list of `n_layers` <span style=\"color:blue\"> decoder blocks</span>.\n",
    "- <span style=\"color:blue\"> [tl.Serial](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial): </span> takes in the following layers or lists of layers:\n",
    "    - <span style=\"color:blue\"> [tl.ShiftRight](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.ShiftRight): </span>: shift the tensor to the right by padding on axis 1.\n",
    "    - <span style=\"color:blue\"> positional_encoder </span>: encodes the text positions.\n",
    "    - <span style=\"color:blue\"> decoder_blocks </span>: the ones you created.\n",
    "    - <span style=\"color:blue\"> [tl.LayerNorm](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm) </span>: a layer norm.\n",
    "    - <span style=\"color:blue\"> [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense) </span>: takes in the vocab_size.\n",
    "    - <span style=\"color:blue\"> [tl.LogSoftmax](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax) </span>: to predict.\n",
    "    \n",
    "Go go go!! You can do it :)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0yi4LJO1RWRS"
   },
   "outputs": [],
   "source": [
    "# UNQ_C7\n",
    "# GRADED FUNCTION: TransformerLM\n",
    "def TransformerLM(vocab_size=33300,\n",
    "                  d_model=512,\n",
    "                  d_ff=2048,\n",
    "                  n_layers=6,\n",
    "                  n_heads=8,\n",
    "                  dropout=0.1,\n",
    "                  max_len=4096,\n",
    "                  mode='train',\n",
    "                  ff_activation=tl.Relu):\n",
    "    \"\"\"Returns a Transformer language model.\n",
    "\n",
    "    The input to the model is a tensor of tokens. (This model uses only the\n",
    "    decoder part of the overall Transformer.)\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): vocab size.\n",
    "        d_model (int):  depth of embedding.\n",
    "        d_ff (int): depth of feed-forward layer.\n",
    "        n_layers (int): number of decoder layers.\n",
    "        n_heads (int): number of attention heads.\n",
    "        dropout (float): dropout rate (how much to drop out).\n",
    "        max_len (int): maximum symbol length for positional encoding.\n",
    "        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\n",
    "        ff_activation (function): the non-linearity in feed-forward layer.\n",
    "\n",
    "    Returns:\n",
    "        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\n",
    "        to activations over a vocab set.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # Embedding inputs and positional encoder\n",
    "    positional_encoder = [ \n",
    "        # Add embedding layer of dimension (vocab_size, d_model)\n",
    "        tl.Embedding(vocab_size, d_model),\n",
    "        # Use dropout with rate and mode specified\n",
    "        tl.Dropout(rate=dropout, mode=mode),\n",
    "        # Add positional encoding layer with maximum input length and mode specified\n",
    "        tl.PositionalEncoding(max_len=max_len, mode=mode)]\n",
    "\n",
    "    # Create stack (list) of decoder blocks with n_layers with necessary parameters\n",
    "    decoder_blocks = [ \n",
    "        DecoderBlock(d_model, d_ff, n_heads,\n",
    "                    dropout, mode, ff_activation) for _ in range(n_layers)]\n",
    "\n",
    "    # Create the complete model as written in the figure\n",
    "    return tl.Serial(\n",
    "        # Use teacher forcing (feed output of previous step to current step)\n",
    "        tl.ShiftRight(mode=mode), # Specify the mode!\n",
    "        # Add positional encoder\n",
    "        positional_encoder,\n",
    "        # Add decoder blocks\n",
    "        decoder_blocks,\n",
    "        # Normalize layer\n",
    "        tl.LayerNorm(),\n",
    "\n",
    "        # Add dense layer of vocab_size (since need to select a word to translate to)\n",
    "        # (a.k.a., logits layer. Note: activation already set by ff_activation)\n",
    "        tl.Dense(vocab_size),\n",
    "        # Get probabilities with Logsoftmax\n",
    "        tl.LogSoftmax(),\n",
    "    )\n",
    "\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial[\n",
      "  ShiftRight(1)\n",
      "  Embedding_33300_512\n",
      "  Dropout\n",
      "  PositionalEncoding\n",
      "  Serial[\n",
      "    Branch_out2[\n",
      "      None\n",
      "      Serial[\n",
      "        LayerNorm\n",
      "        Serial[\n",
      "          Branch_out3[\n",
      "            [Dense_512, AttnHeads]\n",
      "            [Dense_512, AttnHeads]\n",
      "            [Dense_512, AttnHeads]\n",
      "          ]\n",
      "          DotProductAttn_in3\n",
      "          AttnOutput\n",
      "          Dense_512\n",
      "        ]\n",
      "        Dropout\n",
      "      ]\n",
      "    ]\n",
      "    Add_in2\n",
      "  ]\n",
      "  Serial[\n",
      "    Branch_out2[\n",
      "      None\n",
      "      Serial[\n",
      "        LayerNorm\n",
      "        Dense_2048\n",
      "        Relu\n",
      "        Dropout\n",
      "        Dense_512\n",
      "        Dropout\n",
      "      ]\n",
      "    ]\n",
      "    Add_in2\n",
      "  ]\n",
      "  LayerNorm\n",
      "  Dense_33300\n",
      "  LogSoftmax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the Transformer\n",
    "print(TransformerLM(n_layers=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    "Serial[\n",
    "  ShiftRight(1)\n",
    "  Embedding_33300_512\n",
    "  Dropout\n",
    "  PositionalEncoding\n",
    "  Serial[\n",
    "    Branch_out2[\n",
    "      None\n",
    "      Serial[\n",
    "        LayerNorm\n",
    "        Serial[\n",
    "          Branch_out3[\n",
    "            [Dense_512, AttnHeads]\n",
    "            [Dense_512, AttnHeads]\n",
    "            [Dense_512, AttnHeads]\n",
    "          ]\n",
    "          DotProductAttn_in3\n",
    "          AttnOutput\n",
    "          Dense_512\n",
    "        ]\n",
    "        Dropout\n",
    "      ]\n",
    "    ]\n",
    "    Add_in2\n",
    "  ]\n",
    "  Serial[\n",
    "    Branch_out2[\n",
    "      None\n",
    "      Serial[\n",
    "        LayerNorm\n",
    "        Dense_2048\n",
    "        Relu\n",
    "        Dropout\n",
    "        Dense_512\n",
    "        Dropout\n",
    "      ]\n",
    "    ]\n",
    "    Add_in2\n",
    "  ]\n",
    "  LayerNorm\n",
    "  Dense_33300\n",
    "  LogSoftmax\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dRRKnoAdvmJ7"
   },
   "source": [
    "<a name='3'></a>\n",
    "# Part 3: Training\n",
    "\n",
    "Now you are going to train your model. As usual, you have to define the cost function, the optimizer, and decide whether you will be training it on a `gpu` or `cpu`. In this case, you will train your model on a cpu for a few steps and we will load in a pre-trained model that you can use to predict with your own words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l1lkVebQRWRV"
   },
   "source": [
    "<a name='3.1'></a>\n",
    "### 3.1 Training the model\n",
    "\n",
    "You will now write a function that takes in your model and trains it. To train your model you have to decide how many times you want to iterate over the entire data set. Each iteration is defined as an `epoch`. For each epoch, you have to go over all the data, using your training iterator.\n",
    "\n",
    "<a name='ex05'></a>\n",
    "### Exercise 05\n",
    "**Instructions:** Implement the `train_model` program below to train the neural network above. Here is a list of things you should do:\n",
    "\n",
    "- Create the train task by calling [`trax.supervised.training.TrainTask`](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask) and pass in the following: \n",
    "    - <span style='color:blue'> labeled_data </span> = train_gen\n",
    "    - <span style='color:blue'> loss_fn </span> = [tl.CrossEntropyLoss()](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.CrossEntropyLoss)\n",
    "    - <span style='color:blue'> optimizer </span> = [trax.optimizers.Adam(0.01)](https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html#trax.optimizers.adam.Adam)\n",
    "    - <span style='color:blue'> lr_schedule </span> = [lr_schedule](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.lr_schedules.warmup_and_rsqrt_decay)\n",
    "\n",
    "\n",
    "- Create the eval task by calling [`trax.supervised.training.EvalTask`](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask) and pass in the following: \n",
    "    - <span style='color:blue'> labeled_data </span> = eval_gen\n",
    "    - <span style='color:blue'> metrics </span> = tl.CrossEntropyLoss() and [tl.Accuracy()](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.Accuracy)\n",
    "    \n",
    "    \n",
    "- Create the training loop by calling [`trax.supervised.Training.Loop`](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop) and pass in the following: \n",
    "    - <span style='color:blue'> TransformerLM </span> \n",
    "    - <span style='color:blue'> train_task </span> \n",
    "    - <span style='color:blue'> eval_task </span> = [eval_task]\n",
    "    - <span style='color:blue'> output_dir</span> = output_dir\n",
    "    \n",
    "You will be using a cross entropy loss, with Adam optimizer. Please read the [Trax](https://trax-ml.readthedocs.io/en/latest/index.html) documentation to get a full understanding. \n",
    "\n",
    "The training loop that this function returns can be runned using the `run()` method by passing in the desired number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gM2gpu4xvjtX"
   },
   "outputs": [],
   "source": [
    "from trax.supervised import training\n",
    "\n",
    "# UNQ_C8\n",
    "# GRADED FUNCTION: train_model\n",
    "def training_loop(TransformerLM, train_gen, eval_gen, output_dir = \"~/model\"):\n",
    "    '''\n",
    "    Input:\n",
    "        TransformerLM (trax.layers.combinators.Serial): The model you are building.\n",
    "        train_gen (generator): Training stream of data.\n",
    "        eval_gen (generator): Evaluation stream of data.\n",
    "        output_dir (str): folder to save your file.\n",
    "        \n",
    "    Returns:\n",
    "        trax.supervised.training.Loop: Training loop.\n",
    "    '''\n",
    "    output_dir = os.path.expanduser(output_dir)  # trainer is an object\n",
    "    lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=1000, max_value=0.01)\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    train_task = training.TrainTask( \n",
    "      labeled_data=train_gen, # The training generator\n",
    "      loss_layer=tl.CrossEntropyLoss(), # Loss function \n",
    "      optimizer=trax.optimizers.Adam(0.01), # Optimizer (Don't forget to set LR to 0.01)\n",
    "      lr_schedule=lr_schedule,\n",
    "      n_steps_per_checkpoint=10\n",
    "    )\n",
    "\n",
    "    eval_task = training.EvalTask( \n",
    "      labeled_data=eval_gen, # The evaluation generator\n",
    "      metrics=[tl.CrossEntropyLoss(), tl.Accuracy()] # CrossEntropyLoss and Accuracy\n",
    "    )\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    loop = training.Loop(TransformerLM(d_model=4,\n",
    "                                       d_ff=16,\n",
    "                                       n_layers=1,\n",
    "                                       n_heads=2,\n",
    "                                       mode='train'),\n",
    "                         train_task,\n",
    "                         eval_tasks=[eval_task],\n",
    "                         output_dir=output_dir)\n",
    "    \n",
    "    return loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the model will be trained for only 10 steps. \n",
    "\n",
    "Even with this constraint the model with the original default arguments took a very long time to finish. Because of this some parameters are changed when defining the model that is fed into the training loop in the function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "BFRBTwSqRWRZ",
    "outputId": "aff859e5-8f4a-4d3b-f1d3-98e137581a77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step      1: Ran 1 train steps in 8.21 secs\n",
      "Step      1: train CrossEntropyLoss |  10.41494083\n",
      "Step      1: eval  CrossEntropyLoss |  10.41660213\n",
      "Step      1: eval          Accuracy |  0.00000000\n",
      "\n",
      "Step     10: Ran 9 train steps in 51.00 secs\n",
      "Step     10: train CrossEntropyLoss |  10.41452980\n",
      "Step     10: eval  CrossEntropyLoss |  10.41239834\n",
      "Step     10: eval          Accuracy |  0.00000000\n"
     ]
    }
   ],
   "source": [
    "# Should take around 1.5 minutes\n",
    "!rm -f ~/model/model.pkl.gz\n",
    "loop = training_loop(TransformerLM, train_batch_stream, eval_batch_stream)\n",
    "loop.run(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XKrEBjmskeWa"
   },
   "source": [
    " <a name='4'></a>\n",
    " # Part 4:  Evaluation  \n",
    "\n",
    "<a name='4.1'></a>\n",
    "### 4.1 Loading in a trained model\n",
    "\n",
    "In this part you will evaluate by loading in an almost exact version of the model you coded, but we trained it for you to save you time. Please run the cell below to load in the model.\n",
    "\n",
    "As you may have already noticed the model that you trained and the pretrained model share the same overall architecture but they have different values for some of the parameters:\n",
    "\n",
    "    \n",
    "   `Original (pretrained) model: `                                 \n",
    "                                       \n",
    "    TransformerLM(vocab_size=33300, d_model=512, d_ff=2048, n_layers=6, n_heads=8, \n",
    "                   dropout=0.1, max_len=4096, ff_activation=tl.Relu)\n",
    "                   \n",
    "   `Your model:`\n",
    "   \n",
    "    TransformerLM(d_model=4, d_ff=16, n_layers=1, n_heads=2)\n",
    "   \n",
    "   **Only the parameters shown for your model were changed. The others stayed the same.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "zWoSzR5tkoAx",
    "outputId": "2b9f1cca-4778-4509-bd9e-bd1738625a4e"
   },
   "outputs": [],
   "source": [
    "# Get the model architecture\n",
    "model = TransformerLM(mode='eval')\n",
    "\n",
    "# Load the pre-trained weights\n",
    "model.init_from_file('model.pkl.gz', weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ilM9C8P3RWRf"
   },
   "source": [
    "<a name='5'></a>\n",
    "# Part 5: Testing with your own input\n",
    "\n",
    "You will now test your input. You are going to implement greedy decoding. This consists of two functions. The first one allows you to identify the next symbol. It gets the argmax of the output of your model and then returns that index. \n",
    "\n",
    "<a name='ex06'></a>\n",
    "### Exercise 06\n",
    "**Instructions:** Implement the next symbol function that takes in the cur_output_tokens and the trained model to return the index of the next word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rD_bXRCpRWRg"
   },
   "outputs": [],
   "source": [
    "# UNQ_C9\n",
    "def next_symbol(cur_output_tokens, model):\n",
    "    \"\"\"Returns the next symbol for a given sentence.\n",
    "\n",
    "    Args:\n",
    "        cur_output_tokens (list): tokenized sentence with EOS and PAD tokens at the end.\n",
    "        model (trax.layers.combinators.Serial): The transformer model.\n",
    "\n",
    "    Returns:\n",
    "        int: tokenized symbol.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # current output tokens length\n",
    "    token_length = len(cur_output_tokens)\n",
    "    # calculate the minimum power of 2 big enough to store token_length\n",
    "    # HINT: use np.ceil() and np.log2()\n",
    "    # add 1 to token_length so np.log2() doesn't receive 0 when token_length is 0\n",
    "    padded_length = 2**int(np.ceil(np.log2(token_length + 1)))\n",
    "\n",
    "    # Fill cur_output_tokens with 0's until it reaches padded_length\n",
    "    padded = cur_output_tokens + [0] * (padded_length - token_length)\n",
    "    padded_with_batch = np.array(padded)[None, :] # Don't replace this 'None'! This is a way of setting the batch dim\n",
    "\n",
    "    # model expects a tuple containing two padded tensors (with batch)\n",
    "    output, _ = model((padded_with_batch, padded_with_batch)) \n",
    "    # HINT: output has shape (1, padded_length, vocab_size)\n",
    "    # To get log_probs you need to index output with 0 in the first dim\n",
    "    # token_length in the second dim and all of the entries for the last dim.\n",
    "    \n",
    "    log_probs = output[0, token_length, :]\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return int(np.argmax(log_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test it out!\n",
    "sentence_test_nxt_symbl = \"I want to fly in the sky.\"\n",
    "detokenize([next_symbol(tokenize(sentence_test_nxt_symbl)+[0], model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    "'The'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2AwrQFglRWRj"
   },
   "source": [
    "<a name='5.1'></a>\n",
    "### 5.1 Greedy decoding\n",
    "\n",
    "Now you will implement the greedy_decode algorithm that will call the `next_symbol` function. It takes in the input_sentence, the trained model and returns the decoded sentence. \n",
    "\n",
    "<a name='ex07'></a>\n",
    "### Exercise 07\n",
    "\n",
    "**Instructions**: Implement the greedy_decode algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6HwIdimiN0k2"
   },
   "outputs": [],
   "source": [
    "# UNQ_C10\n",
    "# Decoding functions.\n",
    "def greedy_decode(input_sentence, model):\n",
    "    \"\"\"Greedy decode function.\n",
    "\n",
    "    Args:\n",
    "        input_sentence (string): a sentence or article.\n",
    "        model (trax.layers.combinators.Serial): Transformer model.\n",
    "\n",
    "    Returns:\n",
    "        string: summary of the input.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    # Use tokenize()\n",
    "    cur_output_tokens = tokenize(input_sentence) + [0]\n",
    "    generated_output = [] \n",
    "    cur_output = 0 \n",
    "    EOS = 1 \n",
    "    \n",
    "    while cur_output != EOS:\n",
    "        # Get next symbol\n",
    "        cur_output = next_symbol(cur_output_tokens, model)\n",
    "        # Append next symbol to original sentence\n",
    "        cur_output_tokens.append(cur_output)\n",
    "        # Append next symbol to generated sentence\n",
    "        generated_output.append(cur_output)\n",
    "        print(detokenize(generated_output))\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return detokenize(generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "9kHuIDGW1sOr",
    "outputId": "2525ca2c-4625-47c0-8456-f75598581993"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a sunny day when I went to the market to buy some flowers. But\n",
      "I only found roses, not tulips. \n",
      "\n",
      ":\n",
      ": I\n",
      ": I just\n",
      ": I just found\n",
      ": I just found ros\n",
      ": I just found roses\n",
      ": I just found roses,\n",
      ": I just found roses, not\n",
      ": I just found roses, not tu\n",
      ": I just found roses, not tulips\n",
      ": I just found roses, not tulips\n",
      ": I just found roses, not tulips.\n",
      ": I just found roses, not tulips.<EOS>\n",
      ": I just found roses, not tulips.<EOS>\n"
     ]
    }
   ],
   "source": [
    "# Test it out on a sentence!\n",
    "test_sentence = \"It was a sunny day when I went to the market to buy some flowers. But I only found roses, not tulips.\"\n",
    "print(wrapper.fill(test_sentence), '\\n')\n",
    "print(greedy_decode(test_sentence, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CA-279WI2D3G"
   },
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    ":\n",
    ": I\n",
    ": I just\n",
    ": I just found\n",
    ": I just found ros\n",
    ": I just found roses\n",
    ": I just found roses,\n",
    ": I just found roses, not\n",
    ": I just found roses, not tu\n",
    ": I just found roses, not tulips\n",
    ": I just found roses, not tulips\n",
    ": I just found roses, not tulips.\n",
    ": I just found roses, not tulips.<EOS>\n",
    ": I just found roses, not tulips.<EOS>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DYgX-mzjyUia",
    "outputId": "b901e164-48b3-4124-d21a-fe7443d15b79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It’s the posing craze sweeping the U.S. after being brought to fame by\n",
      "skier Lindsey Vonn, soccer star Omar Cummings, baseball player Albert\n",
      "Pujols - and even Republican politician Rick Perry. But now four\n",
      "students at Riverhead High School on Long Island, New York, have been\n",
      "suspended for dropping to a knee and taking up a prayer pose to mimic\n",
      "Denver Broncos quarterback Tim Tebow. Jordan Fulcoly, Wayne Drexel,\n",
      "Tyler Carroll and Connor Carroll were all suspended for one day\n",
      "because the ‘Tebowing’ craze was blocking the hallway and presenting a\n",
      "safety hazard to students. Scroll down for video. Banned: Jordan\n",
      "Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll (all pictured\n",
      "left) were all suspended for one day by Riverhead High School on Long\n",
      "Island, New York, for their tribute to Broncos quarterback Tim Tebow.\n",
      "Issue: Four of the pupils were suspended for one day because they\n",
      "allegedly did not heed to warnings that the 'Tebowing' craze at the\n",
      "school was blocking the hallway and presenting a safety hazard to\n",
      "students. \n",
      "\n",
      "Jordan\n",
      "Jordan Ful\n",
      "Jordan Fulcol\n",
      "Jordan Fulcoly\n",
      "Jordan Fulcoly,\n",
      "Jordan Fulcoly, Wayne\n",
      "Jordan Fulcoly, Wayne Dre\n",
      "Jordan Fulcoly, Wayne Drexe\n",
      "Jordan Fulcoly, Wayne Drexel\n",
      "Jordan Fulcoly, Wayne Drexel,\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day.\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not hee\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warn\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the '\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Te\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebow\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "cra\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocki\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hall\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting a\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting a safety\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting a safety hazard\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting a safety hazard to\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting a safety hazard to\n",
      "students\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting a safety hazard to\n",
      "students.\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting a safety hazard to\n",
      "students.<EOS>\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting a safety hazard to\n",
      "students.<EOS>\n"
     ]
    }
   ],
   "source": [
    "# Test it out with a whole article!\n",
    "article = \"It’s the posing craze sweeping the U.S. after being brought to fame by skier Lindsey Vonn, soccer star Omar Cummings, baseball player Albert Pujols - and even Republican politician Rick Perry. But now four students at Riverhead High School on Long Island, New York, have been suspended for dropping to a knee and taking up a prayer pose to mimic Denver Broncos quarterback Tim Tebow. Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were all suspended for one day because the ‘Tebowing’ craze was blocking the hallway and presenting a safety hazard to students. Scroll down for video. Banned: Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll (all pictured left) were all suspended for one day by Riverhead High School on Long Island, New York, for their tribute to Broncos quarterback Tim Tebow. Issue: Four of the pupils were suspended for one day because they allegedly did not heed to warnings that the 'Tebowing' craze at the school was blocking the hallway and presenting a safety hazard to students.\"\n",
    "print(wrapper.fill(article), '\\n')\n",
    "print(greedy_decode(article, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    "Jordan\n",
    "Jordan Ful\n",
    "Jordan Fulcol\n",
    "Jordan Fulcoly\n",
    "Jordan Fulcoly,\n",
    "Jordan Fulcoly, Wayne\n",
    "Jordan Fulcoly, Wayne Dre\n",
    "Jordan Fulcoly, Wayne Drexe\n",
    "Jordan Fulcoly, Wayne Drexel\n",
    "Jordan Fulcoly, Wayne Drexel,\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "Final summary:\n",
    "\n",
    "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
    "suspended for one day. Four students were suspended for one day\n",
    "because they allegedly did not heed to warnings that the 'Tebowing'\n",
    "craze was blocking the hallway and presenting a safety hazard to\n",
    "students.<EOS>\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "NLPC4-2"
   ]
  },
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
